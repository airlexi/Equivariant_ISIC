{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import math\n",
    "import e2cnn.nn as enn\n",
    "from e2cnn.nn import init\n",
    "from e2cnn import gspaces\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import albumentations as A\n",
    "import torchtoolbox.transform as transforms\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import sklearn\n",
    "from tqdm.autonotebook import tqdm\n",
    "import isic_train\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def predict(model, batch_size):\n",
    "    \"\"\"function to predict test samples with test time augmentation \"\"\"\n",
    "    #loading the dataset\n",
    "    test_set = isic_train.MelanomaDataset(df=test_df,\n",
    "                       imfolder='test', \n",
    "                       train=False,\n",
    "                       transforms=train_transform,\n",
    "                       meta_features=meta_features)\n",
    "    \n",
    "    #initializing the dataloader\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    length_dataset = len(test_set)\n",
    "    \n",
    "    #number of test time augmentation\n",
    "    number_of_tta = 5\n",
    "    \n",
    "    #setting model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    #initializing the variable for test predictions\n",
    "    test_preds = torch.zeros(size = (length_dataset, 1), device=\"cpu\", dtype=torch.float32)\n",
    "    \n",
    "    for _ in range(number_of_tta):\n",
    "        with torch.no_grad():\n",
    "            for k, (images) in tqdm(enumerate(test_loader), total=int(torch.ceil(torch.tensor(length_dataset / batch_size)).item())):\n",
    "\n",
    "                images[0] = images[0].to(device)\n",
    "                images[1] = images[1].to(device)\n",
    "\n",
    "                out = model(images)\n",
    "                pred = torch.sigmoid(out)\n",
    "                \n",
    "                #writing the predictions to the corresponding variable\n",
    "                test_preds[test_loader.batch_size*k : k*test_loader.batch_size+ images[0].shape[0]] += pred.cpu()\n",
    "\n",
    "    final_pred = test_preds/number_of_tta\n",
    "    return final_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File test.csv does not exist: 'test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8ed5a79f07e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File test.csv does not exist: 'test.csv'"
     ]
    }
   ],
   "source": [
    "#loading the .csv files with meta_data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "# data augmentation for test time augmentation\n",
    "# make sure you use the same augmentations you used in training\n",
    "test_transform = transforms.Compose([\n",
    "    isic_train.AdvancedHairAugmentation(hairs_folder='mel_hairs'),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomAffine(degrees=0, scale=(0.8,1.2), shear=(-20,20)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    isic_train.Microscope(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# preprocessing of meta data, taken from https://www.kaggle.com/nroman/melanoma-pytorch-starter-efficientnet\n",
    "# One-hot encoding of location of imaged site\n",
    "concat = pd.concat([train_df['anatom_site_general_challenge'], test_df['anatom_site_general_challenge']], ignore_index=True)\n",
    "dummies = pd.get_dummies(concat, dummy_na=True, dtype=np.uint8, prefix='site')\n",
    "train_df = pd.concat([train_df, dummies.iloc[:train_df.shape[0]]], axis=1)\n",
    "test_df = pd.concat([test_df, dummies.iloc[train_df.shape[0]:].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# encoding the sex of patients, -1 if it is missing\n",
    "train_df['sex'] = train_df['sex'].map({'male': 1, 'female': 0})\n",
    "test_df['sex'] = test_df['sex'].map({'male': 1, 'female': 0})\n",
    "train_df['sex'] = train_df['sex'].fillna(-1)\n",
    "test_df['sex'] = test_df['sex'].fillna(-1)\n",
    "\n",
    "# normalizing the age of the patients between 0 and 1 to use it for classification as well, 0 if the age is missing\n",
    "train_df['age_approx'] /= train_df['age_approx'].max()\n",
    "test_df['age_approx'] /= test_df['age_approx'].max()\n",
    "train_df['age_approx'] = train_df['age_approx'].fillna(0)\n",
    "test_df['age_approx'] = test_df['age_approx'].fillna(0)\n",
    "\n",
    "# filling missing values for patient_ids\n",
    "train_df['patient_id'] = train_df['patient_id'].fillna(0)\n",
    "meta_features = ['sex', 'age_approx'] + [col for col in train_df.columns if 'site_' in col]\n",
    "meta_features.remove('anatom_site_general_challenge')\n",
    "\n",
    "\n",
    "#initialize the model you want to make your submission with and load the weights and predict the test samples\n",
    "#first fold\n",
    "model = isic_train.DenseNet(32, [6,12,24,16], len(meta_features)).to(device)\n",
    "weights = torch.load(\"best_model_fold_1\")\n",
    "model.load_state_dict(weights, strict=True)\n",
    "prediction_fold_1 = predict(model, 256)\n",
    "\n",
    "#second fold\n",
    "model = isic_train.DenseNet(32, [6,12,24,16], len(meta_features)).to(device)\n",
    "weights = torch.load(\"best_model_fold_2\")\n",
    "model.load_state_dict(weights, strict=True)\n",
    "prediction_fold_2 = predict(model, 256)\n",
    "\n",
    "#third fold\n",
    "model = isic_train.DenseNet(32, [6,12,24,16], len(meta_features)).to(device)\n",
    "weights = torch.load(\"best_model_fold_3\")\n",
    "model.load_state_dict(weights, strict=True)\n",
    "prediction_fold_3 = predict(model, 256)\n",
    "\n",
    "#fourth fold\n",
    "model = isic_train.DenseNet(32, [6,12,24,16], len(meta_features)).to(device)\n",
    "weights = torch.load(\"best_model_fold_4\")\n",
    "model.load_state_dict(weights, strict=True)\n",
    "prediction_fold_4 = predict(model, 256)\n",
    "\n",
    "#fifth fold\n",
    "model = isic_train.DenseNet(32, [6,12,24,16], len(meta_features)).to(device)\n",
    "weights = torch.load(\"best_model_fold_5\")\n",
    "model.load_state_dict(weights, strict=True)\n",
    "prediction_fold_5 = predict(model, 256)\n",
    "\n",
    "#summing up the predictions of each fold and averaging them\n",
    "final_prediction = (prediction_fold_1 + prediction_fold_2 + prediction_fold_3 + prediction_fold_4 + prediction_fold_5)/5\n",
    "\n",
    "#writing the final_predictions to the submission.csv file\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['target'] = final_prediction.cpu().numpy().reshape(-1,)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
